<!DOCTYPE html>
<html lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>Miroslav Bachinski</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="Personal Page of Miroslav Bachinski">
	<meta name="author" content="Miroslav Bachinski">
	<!-- CSS imports -->
	<link rel="shortcut icon" href="https://bachinski.de/favicon.ico">
	<link rel="stylesheet" href="./files/bootstrap.min.css">
	<link rel="stylesheet" href="./files/my_style.css">
</head>
<body>
	<div class="container">
		<div class="row clearfix">
			<div class="col-md-6 col-xs-6 column">
				<img class="img-responsive" src="./files/uib_logo.png" alt="Bergen University Logo" width="600">
			</div>
		</div>
		<div class="row clearfix">
			<div class="col-md-12">
				<div id="ubtbody" class="col-md-12">
					<div class="col-md-12" style="height:20px"></div>
					<div id="ubtcontent" class="col-md-12 column">
						<!-- About me -->
						<div class="panel panel-default">
							<div class="panel-body">
								<div class="col-md-4" style="text-align:center">
									<img src="./files/photo.png" class="img-responsive" alt="Miroslav Bachinski"><br>
										<div class="col-md-4" style="width:100%;height:20%;text-align:center">
											<table style="text-align:center;width:100%" cellpadding="10" cellspacing="10" border="0">
												<tbody><tr>
													<td width="50%" align="left"><a href="https://bachinski.de/documents/MiroslavBachinskiCV.pdf" target="_blank"><img style="height:60px;" src="./files/cvicon2.png" border="0"></a></td>
													<td width="50%" align="right"><a href="https://scholar.google.com/citations?user=FZyDfQMAAAAJ&amp;hl=en" target="_blank"><img style="height:60px;" src="./files/gs.png" border="0"></a></td>
												</tr>
											</tbody></table>
										</div>
									</div>
								<div class="col-md-8">
									<h1>Miroslav Bachinski</h1>
									<h4>(old writing: Myroslav Bachynskyi)</h4>
									I'm an Associate Professon in the field of Human-Computer Interaction (HCI) and the <a href="https://hci.uib.no/">HCI research group</a> leader at the <a href="https://www.uib.no/en/infomedia" target="_blank">Department of Information Science and Media Studies</a> at the <a href="https://www.uib.no/" target="_blank">University of Bergen</a>.
									<br>
									<br>
									Before coming to Bergen I was a Postdoctoral researcher at the <a href="https://www.uni-bayreuth.de/en/" target="_blank">University of Bayreuth</a> and at the <a href="https://international.au.dk/" target="_blank">University of Aarhus</a> in the team of <a href="https://www.ai8.uni-bayreuth.de/de/team/Joerg-Mueller">Prof. Jörg Müller</a>, and Postgraduate research visitor at the <a href="https://www.gla.ac.uk/" target="_blank">University of Glasgow</a> in the team of <a href="https://www.dcs.gla.ac.uk/~rod/" >Prof. Roderick Murray-Smith</a>.
									<br>
									<br>
									I was awarded PhD by the the <a href="https://saarland-informatics-campus.de/en/" target="_blank">Department of Computer Science</a> at the <a href="https://www.uni-saarland.de/" target="_blank">University of Saarland</a>, <a href="http://www.mmci.uni-saarland.de/" target="_blank">Cluster of Excellence on "Multimodal Computing and Interaction"</a>, and the <a href="https://www.mpi-inf.mpg.de/" target="_blank">Max Planck Institute for Informatics</a> for the thesis: "Biomechanical models for Human-Computer Interaction", supervised by <a href="https://people.aalto.fi/antti.oulasvirta" target="_blank">Prof. Dr. Antti Oulasvirta</a> and <a href="https://hci.cs.uni-saarland.de/people/juergen-steimle/" target="_blank">Prof. Dr. Jürgen Steimle</a>.
									<br>
									<br>
									My research focuses on development and application of data-driven methods to diverse research and design problems, including improvement of post-desktop user interfaces with large space of alternative designs (e. g. Virtual Reality, Levitation), development of wearable sensor systems, development of digital biomarkers for early diagnosis of dementia, and development of assistive technologies. 
									I adopt machine learning and artificial intelligence technologies, optical motion capture, movement dynamics modeling, biomechanical modeling and simulation besides standard HCI methods to formalize the design space in mathematical models and find optimal designs.
									

								</div>
							</div>
						</div>

							<!-- PUBLICATIONS -->
				<div class="panel panel-default">
					<div class="panel-heading">
						<h3 class="panel-title" id="publications">Research Projects and Publications</h3>
					</div>
					<div class="panel-body"> 
						<h2>Peer-reviewed papers</h2>
					
						<!-- CHI 2020 dynamics -->
						<div class="row">
								<div class="col-md-4" style="text-align:center">
									<img style="width:95%;" src="./files/chi2020dynamics.png">
								</div>
								<div class="col-md-8">
									<span style="font-size:1.2em"><link color="red"> <b>Dynamics of Aimed Mid-air Movements</b></span>
									<br><br>
									M. Bachinski, J. Müller
									<br><br>
									To appear at <b>ACM CHI 2020</b>
									<br><br>
									Mid-air arm movements are ubiquitous in VR, AR, and gestural interfaces. While mouse movements have received some attention, the dynamics of mid-air movements are understudied in HCI. In this paper we present an exploratory analysis of the dynamics of aimed mid-air movements. We explore the 3rd order lag (3OL) and existing 2nd order lag (2OL) models for modeling these dynamics. For a majority of movements the 3OL model captures mid-air dynamics better, in particular acceleration. The models can effectively predict the complete time series of position, velocity and acceleration of aimed movements given an initial state and a target using three (2OL) or four (3OL) free parameters.
									<br><br>
									[<a href="https://bachinski.de/">pdf</a>]	
									</div>
								</div>
						<br><br><br>

						<!-- CHI 2020 throwing -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
							<img style="width:95%;" src="./files/chi2020throwing.png">
						</div>
						<div class="col-md-8">
							<span style="font-size:1.2em"><link color="red"> <b>Performance and Experience of Throwing in Virtual Reality</b></span>
							<br><br>
							T. Zindulka, M. Bachinski, J. Müller
							<br><br>
							To appear at <b>ACM CHI 2020</b>
							<br><br>
							Throwing is a fundamental movement in many sports and games. Given this, accurate throwing in VR applications today is surprisingly difficult. In this paper we explore the nature of the difficulties of throwing in VR in more detail. We present the results of a user study comparing throwing in VR and in the physical world. In a short pre-study with 3 participants we determine an optimal number of throwing repetitions for the main study by exploring the learning curve and subjective fatigue of throwing in VR. In the main study, with 12 participants, we find that throwing precision and accuracy in VR are lower particularly in the distance and height dimensions. It also requires more effort and exhibits different kinematic patterns.
							<br><br>
							[<a href="https://bachinski.de/">pdf</a>]	
				</div>
				</div>
						<br><br><br>

						<!-- CHI 2020 levisim -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
							<img style="width:95%;" src="./files/chi2020levicursorinvr.png">
						</div>
						<div class="col-md-8">
							<span style="font-size:1.2em"><link color="red"> <b>Levitation Simulator: Prototyping Ultrasonic Levitation Interfaces in Virtual Reality</b></span>
							<br><br>
							V. Paneva, M. Bachinski, J. Müller
							<br><br>
							To appear at <b>ACM CHI 2020</b>
							<br><br>
							We present the Levitation Simulator, a system that enables researchers and designers to iteratively develop and prototype levitation interface ideas in Virtual Reality. This includes user tests and formal experiments. We derive a model of the movement of a levitating particle in such an interface. Based on this, we develop an interactive simulation of the levitation interface in VR, which exhibits the dynamical properties of the real interface. The results of a Fitts’ Law pointing study show that the Levitation Simulator enables performance, comparable to the real prototype. We developed the first two interactive games, dedicated for levitation interfaces: LeviShooter and BeadBounce, in the Levitation Simulator, and then implemented them on the real interface. Our results indicate that participants experienced similar levels of user engagement when playing the games, in the two environments. We share our Levitation Simulator as Open Source, thereby democratizing levitation research, without the need for a levitation apparatus.
							<br><br>
							[<a href="https://bachinski.de/">pdf</a>]	
				</div>
	</div>					<br><br><br>
			
						<!-- ISS 2018 -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/iss2018.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>LeviCursor: Dexterous Interaction with a Levitating Object</b></span>
								<br><br>
								M. Bachinski, V. Paneva, J. Müller
								<br><br>
								<b>ACM ISS 2018</b>
								<br><br>
								We present LeviCursor, a method for interactively moving a physical, levitating particle in 3D with high agility. The levitating object can move continuously and smoothly in any direction. We optimize the transducer phases for each possible levitation point independently. Using precomputation, our system can determine the optimal transducer phases within a few microseconds and achieves round-trip latencies of 15 ms. Due to our interpolation scheme, the levitated object can be controlled almost instantaneously with sub-millimeter accuracy. We present a particle stabilization mechanism which ensures the levitating particle is always in the main levitation trap. Lastly, we conduct the first Fitts' law-type pointing study with a real 3D cursor, where participants control the movement of the levitated cursor between two physical targets. The results of the user study demonstrate that using LeviCursor, users reach performance comparable to that of a mouse pointer.
								<br><br>
								[<a href="https://dl.acm.org/doi/10.1145/3279778.3279802">pdf</a>]	
						</div>
						</div>
						<br><br><br>

						<!-- MiG 2016 -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/mig2016.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>Introducing postural variability improves the distribution of muscular loads during mid-air gestural interaction</b></span>
								<br><br>
								F. Nunnari, M. Bachinski, A. Heloir
								<br><br>
								<b>ACM MIG '16: Proceedings of the 9th International Conference on Motion in Games</b>
								<br><br>
								Only time will tell if motion-controlled systems are the future of gaming and other industries and if mid-air gestural input will eventually offer a more intuitive way to play games and interact with computers. Whatever the eventual outcome, it is necessary to assess the ergonomics of mid-air input metaphors and propose design guidelines which will guarantee their safe use in the long run. This paper presents an ergonomic study showing how to mitigate the muscular strain induced by prolonged mid-air gesture interaction by encouraging postural shifts during the interaction. A quantitative and qualitative user study involving 30 subjects validates the setup. The simulated musculo-skeletal load values support our hypothesis and show a statistically significant 19% decrease in average muscle loads on the shoulder, neck, and back area in the modified condition compared to the baseline.
								<br><br>
								[<a href="https://dl.acm.org/doi/abs/10.1145/2994258.2994278">pdf</a>]	
						</div>
						</div>
						<br><br><br>

						<!-- CHI 2015 -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/chi2015.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>Performance and Ergonomics of Touch Surfaces: A Comparative Study using Biomechanical Simulation</b></span>
								<br><br>
								M. Bachynskyi, G. Palmas, A. Oulasvirta, J. Steimle, T. Weinkauf
								<br><br>
								<b>ACM CHI 2015</b>
								<br><br>
								Although different types of touch surfaces have gained extensive attention in HCI, this is the first work to directly compare them for two critical factors: performance and ergonomics. Our data come from a pointing task (N=40) carried out on five common touch surface types: public display (large, vertical, standing), tabletop (large, horizontal, seated), laptop (medium, adjustably tilted, seated), tablet (seated, in hand), and smartphone (single- and two-handed input). Ergonomics indices were calculated from biomechanical simulations of motion capture data combined with recordings of external forces. We provide an extensive dataset for researchers and report the first analyses of similarities and differences that are attributable to the different postures and movement ranges.
								<br><br>
								[<a href="https://dl.acm.org/doi/abs/10.1145/2702123.2702607">pdf</a>]	
						</div>
						</div>
						<br><br><br>

						<!-- ToCHI 2016 -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/tochi2015.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>Informing the Design of Novel Input Methods with Muscle Coactivation Clustering</b></span>
								<br><br>
								M. Bachynskyi, G. Palmas, A. Oulasvirta, T. Weinkauf
								<br><br>
								<b>ACM ToCHI, Volume: 21, Issue: 6, 2015, p. 1-25</b>
								<br><br>
								This article presents a novel summarization of biomechanical and performance data for user interface designers. Previously, there was no simple way for designers to predict how the location, direction, velocity, precision, or amplitude of users’ movement affects performance and fatigue. We cluster muscle coactivation data from a 3D pointing task covering the whole reachable space of the arm. We identify 11 clusters of pointing movements with distinct muscular, spatio-temporal, and performance properties. We discuss their use as heuristics when designing for 3D pointing.
								<br><br>
								[<a href="https://dl.acm.org/doi/abs/10.1145/2687921">pdf</a>]	
						</div>
						</div>
						<br><br><br>
						
						<!-- ASB 2015 -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/asb2015.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>Physical ergonomics of tablet interaction while sitting</b></span>
								<br><br>
								M. Bachynskyi
								<br><br>
								<b>39th Annual Meeting of the American Society of Biomechanics</b>
								<br><br>
								We present physical ergonomics assessment of
								typical tablet device usage. Tablet devices are
								becoming widespread and often even displace
								personal computers and laptops. However, while
								physical ergonomics of PCs and laptops was
								extensively studied in the past, there is only little
								knowledge of ergonomics of tablet devices. In
								particular the user assessment is complex due to
								portability of tablets, which allows a variety of
								tablet locations, orientations and holds which can be
								adopted by users. The purpose of this work was to
								identify typical postures, set of recruited muscles
								and health risks related to the tablet interaction.
								<br><br>
								[<a href="http://asbweb.org/conferences/2015/abstracts/PD5E_3--Physical%20Ergonomics%20Of%20Tablet%20Interaction%20While%20Sitting--(Bachynskyi).pdf">pdf</a>]	
						</div>
						</div>
						<br><br><br>
						
						<!-- ToVCG 2016 -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/movexp2014.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>MovExp: A Versatile Visualization Tool for Human-Computer Interaction Studies with 3D Performance and Biomechanical Data</b></span>
								<br><br>
								G. Palmas, M. Bachynskyi, A. Oulasvirta, H.-P. Seidel, T. Weinkauf
								<br><br>
								<b>IEEE Transactions on Visualization and Computer Graphics, Volume: 20, Issue: 12, Dec. 31 2014</b>
								<br><br>
								In Human-Computer Interaction (HCI), experts seek to evaluate and compare the performance and ergonomics of user interfaces. Recently, a novel cost-efficient method for estimating physical ergonomics and performance has been introduced to HCI. It is based on optical motion capture and biomechanical simulation. It provides a rich source for analyzing human movements summarized in a multidimensional data set. Existing visualization tools do not sufficiently support the HCI experts in analyzing this data. We identified two shortcomings. First, appropriate visual encodings are missing particularly for the biomechanical aspects of the data. Second, the physical setup of the user interface cannot be incorporated explicitly into existing tools. We present MovExp, a versatile visualization tool that supports the evaluation of user interfaces. In particular, it can be easily adapted by the HCI experts to include the physical setup that is being evaluated, and visualize the data on top of it. Furthermore, it provides a variety of visual encodings to communicate muscular loads, movement directions, and other specifics of HCI studies that employ motion capture and biomechanical simulation. In this design study, we follow a problem-driven research approach. Based on a formalization of the visualization needs and the data structure, we formulate technical requirements for the visualization tool and present novel solutions to the analysis needs of the HCI experts. We show the utility of our tool with four case studies from the daily work of our HCI experts.
								<br><br>
								[<a href="https://ieeexplore.ieee.org/abstract/document/6876050">pdf</a>]	
						</div>
						</div>
						<br><br><br>
						
						<!-- CHI 2014 -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/chi2014.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>Is motion capture-based biomechanical simulation valid for HCI studies?: study and implications</b></span>
								<br><br>
								M. Bachynskyi, A. Oulasvirta, G. Palmas, T. Weinkauf
								<br><br>
								<b>ACM CHI 2014</b>
								<br><br>
								Motion-capture-based biomechanical simulation is a non-invasive analysis method that yields a rich description of posture, joint, and muscle activity in human movement. The method is presently gaining ground in sports, medicine, and industrial ergonomics, but it also bears great potential for studies in HCI where the physical ergonomics of a design is important. To make the method more broadly accessible, we study its predictive validity for movements and users typical to studies in HCI. We discuss the sources of error in biomechanical simulation and present results from two validation studies conducted with a state-of-the-art system. Study I tested aimed movements ranging from multitouch gestures to dancing, finding out that the critical limiting factor is the size of movement. Study II compared muscle activation predictions to surface-EMG recordings in a 3D pointing task. The data shows medium-to-high validity that is, however, constrained by some characteristics of the movement and the user. We draw concrete recommendations to practitioners and discuss challenges to developing the method further.
								<br><br>
								[<a href="https://dl.acm.org/doi/abs/10.1145/2556288.2557027">pdf</a>]	
						</div>
						</div>
						<br><br><br>
						
						<!-- CHI 2014 -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/viz2014.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>An edge-bundling layout for interactive parallel coordinates</b></span>
								<br><br>
								G. Palmas, M. Bachynskyi, A. Oulasvirta, H.-P. Seidel, T. Weinkauf
								<br><br>
								<b>IEEE Pacific Visualization Symposium 2014</b>
								<br><br>
								Parallel Coordinates is an often used visualization method for multidimensional data sets. Its main challenges for large data sets are visual clutter and over plotting which hamper the recognition of patterns in the data. We present an edge-bundling method using density-based clustering for each dimension. This reduces clutter and provides a faster overview of clusters and trends. Moreover, it allows rendering the clustered lines using polygons, decreasing rendering time remarkably. In addition, we design interactions to support multidimensional clustering with this method. A user study shows improvements over the classic parallel coordinates plot in two user tasks: correlation estimation and subset tracing.
								<br><br>
								[<a href="https://ieeexplore.ieee.org/abstract/document/6787137">pdf</a>]	
						</div>
						</div>
						<br><br><br>
						
						<h2>PhD thesis</h2>
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/thesis.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>Biomechanical models for human-computer interaction</b></span>
								<br><br>
								<b>Universität des Saarlandes 2018</b>
								<br><br>
								Post-desktop user interfaces, such as smartphones, tablets, interactive tabletops, public displays and mid-air interfaces, already are a ubiquitous part of everyday human life, or have the potential to be. One of the key features of these interfaces is the reduced number or even absence of input movement constraints imposed by a device form-factor. This freedom is advantageous for users, allowing them to interact with computers using more natural limb movements; however, it is a source of 4 issues for research and design of post-desktop interfaces which make traditional analysis methods inefficient: the new movement space is orders of magnitude larger than the one analyzed for traditional desktops; the existing knowledge on post-desktop input methods is sparse and sporadic; the movement space is non-uniform with respect to performance; and traditional methods are ineffective or inefficient in tackling physical ergonomics pitfalls in post-desktop interfaces. These issues lead to the research problem of efficient assessment, analysis and design methods for high-throughput ergonomic post-desktop interfaces. To solve this research problem and support researchers and designers, this thesis proposes efficient experiment- and model-based assessment methods for post-desktop user interfaces. We achieve this through the following contributions: - adopt optical motion capture and biomechanical simulation for HCI experiments as a versatile source of both performance and ergonomics data describing an input method; - identify applicability limits of the method for a range of HCI tasks; - validate the method outputs against ground truth recordings in typical HCI setting; - demonstrate the added value of the method in analysis of performance and ergonomics of touchscreen devices; and - summarize performance and ergonomics of a movement space through a clustering of physiological data. The proposed method successfully deals with the 4 above-mentioned issues of post-desktop input. The efficiency of the methods makes it possible to effectively tackle the issue of large post-desktop movement spaces both at early design stages (through a generic model of a movement space) as well as at later design stages (through user studies). The method provides rich data on physical ergonomics (joint angles and moments, muscle forces and activations, energy expenditure and fatigue), making it possible to solve the issue of ergonomics pitfalls. Additionally, the method provides performance data (speed, accuracy and throughput) which can be related to the physiological data to solve the issue of non-uniformity of movement space. In our adaptation the method does not require experimenters to have specialized expertise, thus making it accessible to a wide range of researchers and designers and contributing towards the solution of the issue of post-desktop knowledge sparsity.
								<br><br>
								[<a href="https://publikationen.sulb.uni-saarland.de/handle/20.500.11880/26731">pdf</a>]	
						</div>
						</div>
						<br><br><br>
						
						<h2>Book chapters, workshops, and other publications</h2>
						<!-- Handbook chapter -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/handbook2019.png">
							</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>Ergonomics for the design of multimodal interfaces</b></span>
								<br><br>
								A. Heloir, F. Nunnari, M. Bachynskyi
								<br><br>
								<b>The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions. Volume 3, July 2019, p. 263–304</b>
								<br><br>
								This chapter presents an overview of ergonomic studies in multimodal interfaces, starting with the main challenges posed by postdesktop interfaces and introducing the importance of ergonomic studies. Then,
	physical ergonomics is introduced. Next, the focus is placed on motion capturebased biomechanical simulation as a universal method for asseing multimodal
	interaction design. Finally, open questions and prospects are discussed.
								<br><br>
								[<a href="https://dl.acm.org/doi/abs/10.1145/3233795.3233804">pdf</a>]	
						</div>
						</div>
						<br><br><br>

						<!-- Workshop CHI 2019 -->
						<div class="row">
							<div class="col-md-4" style="text-align:center">
								<img style="width:95%;" src="./files/hapticworkshop2019.png">
						</div>
							<div class="col-md-8">
								<span style="font-size:1.2em"><link color="red"> <b>Mid-Air Haptic Interfaces for Interactive Digital Signage and Kiosks</b></span>
								<br><br>
								O. Georgiou, H. Limerick, L. Corenthy, M. Perry, M. Maksymenko, S. Frish, J. Müller, M. Bachynskyi, J. R. Kim
								<br><br>
								<b>Extended Abstracts of the CHI 2019</b>
								<br><br>
								Digital signage systems are transitioning from static displays to rich, dynamic interactive experiences while new enabling technologies that support these interactions are also evolving. For instance, advances in computer vision and face, gaze, facial expression, body and hand-gesture recognition have enabled new ways of distal interactivity with digital content. Such possibilities are only just being adopted by advertisers and retailers, yet, they face important challenges e.g. the lack of a commonly accepted gesture, facial expressions or call-to-action set. Another common issue here is the absence of active tactile stimuli. Mid-air haptic interfaces can help alleviate these problems and aid in defining a gesture set, informing users about their interaction via haptic feedback loops, and enhancing the overall user experience. This workshop aims to examine the possibilities opened up by these technologies and discuss opportunities in designing the next generation of interactive digital signage kiosks.
								<br><br>
								[<a href="https://dl.acm.org/doi/abs/10.1145/3290607.3299030">pdf</a>]	
						</div>
						</div>
						<br><br><br>

					</div>
				</div>
				
				<!-- Awards and honours -->
				<!--div class="panel panel-default">
					<div class="panel-heading">
						<h3 class="panel-title" id="contact">Contact</h3>
					</div>
					<div class="panel-body">
					</div>
				</div-->

				<!-- Teaching -->
				<div class="panel panel-default">
					<div class="panel-heading">
						<h3 class="panel-title" id="contact">Teaching</h3>
					</div>
					<div class="panel-body">
						<h4>Autumn semester 2024:</h4>
						<ul>
							<li>  
								<span style="font-size:1.2em"><link color="red"><a href="https://mitt.uib.no/courses/47491" target="blank">MIX203: Design for media production</b></a></span><br>
							</li>
						</ul>
						<h4>Spring semester 2024:</h4>
						<ul>
							<li>  
								<span style="font-size:1.2em"><link color="red"><a href="https://mitt.uib.no/courses/46909" target="blank">MIX114: Web development</b></a></span>
							</li>
						</ul>
						<h4>Autumn semester 2023:</h4>
						<ul>
							<li>  
								<span style="font-size:1.2em"><link color="red"><a href="https://mitt.uib.no/courses/41685" target="blank">MIX203: Design for media production</b></a></span><br>
							</li>
						</ul>
						<h4>Spring semester 2023:</h4>
						<ul>
							<li>  
								<span style="font-size:1.2em"><link color="red"><a href="https://mitt.uib.no/courses/38983" target="blank">MIX114: Web development</b></a></span>
							</li>
						</ul>
						<h4>Autumn semester 2022:</h4>
						<ul>
							<li>  
								<span style="font-size:1.2em"><link color="red"><a href="https://mitt.uib.no/courses/36144" target="blank">MIX301: Media Technology: Theory and Development</b></a></span><br>
							</li>
						</ul>
					</div>
				</div>


				<!-- Contact -->
				<div class="panel panel-default">
					<div class="panel-heading">
						<h3 class="panel-title" id="contact">Contact</h3>
					</div>
					<div class="panel-body">
									<table class="table">
										<tbody><tr>
											<td><i class="fa fa-map-marker" aria-hidden="true"></i>&nbsp;office:</td>
											<td>
												<a href="https://link.mazemap.com/qcuUtSNE">Fosswinckels gate 6, Lauritz Meltzers hus<br>
												Office 616<br>
												5007 Bergen<br>
												Norway</a>
											</td>
										</tr>
										<tr>
											<td><i class="fa fa-map-marker" aria-hidden="true"></i>&nbsp;lab & teaching:</td>
											<td>
												<a href="https://link.mazemap.com/Fzr55zTR">Media City Bergen<br>
												Lars Hillesgate 30, floor 2<br>
												Bergen</a>
											</td>
										</tr>									<tr>
											<td><i class="fa fa-envelope" aria-hidden="true"></i>&nbsp;email:</td>
											<td>  
				<a href="mailto:miroslav.bachinski@uib.no">miroslav.bachinski@uib.no</a><br>

			
			</td>
										</tr>
									</tbody></table>
					</div>
				</div>
				

					</div>
				</div>
			</div>
		</div>
	</div>
</body>
</html>